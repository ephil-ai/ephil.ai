---
title: Trustworthy AI
summary: 'Trustworthiness is recognized as a key quality requirement for AI systems as they become increasingly powerful and prevalent. However, trustworthiness depends on properties such as transparency, fairness, reliability, and robustness--many of which remain poorly understood and difficult to implement. [(read more)](/project/trustworthy-ai)'
date: "2021-12-01T00:00:00Z"
profile: false
show_date: false

# For order in list 
weight: 10

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, place an image named `featured.jpg/png` in your page's folder.
# Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width
# Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
# Set `preview_only` to `true` to just use the image for thumbnails.
image:
 caption: "Image created with [DALL·E by OpenAI](https://openai.com/dall-e-3)"
 placement: 1
 focal_point: Smart
 preview_only: false


links:
#- icon: twitter
 # icon_pack: fab
 # name: Follow
 # url: https://twitter.com/georgecushen
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ''
---

Trustworthiness is a quality requirement for increasingly powerful and prominent AI systems. Trustworthiness depends on a variety of properties such as transparency, fairness, reliability, and robustness, however, many of which remain poorly understood and difficult to implement. For example, it remains unclear to what extent transparency can actually be achieved in high dimensional nonlinear systems, what fairness actually consists in, and how we can assess the reliability of AI-generated content. To a large extent, these are philosophical questions that require close analysis of relevant concepts, as well as a realistic assessment of current technical capabilities against normative and societal constraints.

**Transparency:** ECPAI members Carlos Zednik, Philippe Verreault-Julien, Céline Budding, and Yeji Streppel are engaged in several projects to specify norms and standards of explainability and transparency. To this end, they actively collaborate with AI researchers and       industry representatives to develop, implement, and evaluate methods in explainable AI in domains such as medical decision-making, automated driving, and language-production. They also participate in national and international efforts to promote explainability and transparency through regulatory means such as standardization.

**Reliability:** Elizabeth O'Neill aims to uncover and characterize the criteria that can be used to judge whether or not an AI system’s recommendations should be considered reliable. In particular, her NWO-funded research projects consider the reliability of AI systems capable of moral reasoning.

**Fairness & Trust:** ECPAI members Philip Nickel and Patrik Hummel investigate the conditions for trustworthiness in data-driven decision-making, with a particular focus on medical contexts. To what extent can and should AI systems be trusted to make high-stakes medical decisions? What role does fairness play in the medical decision-making and access to state-of-the-art AI?


## Funded Projects

- [NWO LTP ROBUST project](https://www.tue.nl/en/storage/biomedische-technologie/de-faculteit/news-and-events/news-overview/05-01-2023-robust-ai-program-receives-additional-eur25-million-in-funding-from-dutch-research-council) on explainability of machine vision for self-driving cars
- [NWO XS grant](https://www.tue.nl/en/news-and-events/news-overview/03-05-2023-xs-grant-for-elizabeth-oneill) on "When Computers Join the Moral Conversation"


## Activities

- Carlos Zednik and Yeji Streppel contributed to technical standard [DIN SPEC 92001-3 - Explainability](https://www.beuth.de/en/technical-rule/din-spec-92001-3/369799101)
- Elizabeth O'Neill developed a YouTube video on [*How to Evaluate the Reliability of a Source?*](https://www.youtube.com/watch?v=Ts8F4V0yI-M)
- Vlasta Sikimić participated in the [governmental advisory board creating the Ethical Guidelines for Safe and Reliable Use of AI in the Republic of Serbia](https://www.ai.gov.rs/vest/en/423/adopted-ethical-guidelines-for-safe-and-reliable-use-of-ai.php). 


## Associated Researchers

- Céline Budding
- Elizabeth O'Neill
- Philip J. Nickel
- Yeji Streppel
- Philippe Verreault-Julien
- Carlos Zednik
