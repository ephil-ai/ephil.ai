---
title: Trustworthy AI
summary: 'Trustworthiness is recognized as a key quality requirement for AI systems as they become increasingly powerful and prevalent. However, trustworthiness depends on properties such as transparency, fairness, reliability, and robustness--many of which remain poorly understood and difficult to implement. [(read more)](/project/trustworthy-ai)'
date: "2021-12-01T00:00:00Z"
profile: false
show_date: false

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, place an image named `featured.jpg/png` in your page's folder.
# Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width
# Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
# Set `preview_only` to `true` to just use the image for thumbnails.
image:
 caption: "Image created with [DALL·E by OpenAI](https://openai.com/dall-e-3)"
 placement: 1
 focal_point: Smart
 preview_only: false


links:
#- icon: twitter
 # icon_pack: fab
 # name: Follow
 # url: https://twitter.com/georgecushen
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ''
---

Trustworthiness is recognized as a key quality requirement for AI systems as they become increasingly powerful and prevalent. However, trustworthiness depends on properties such as transparency, fairness, reliability, and robustness--many of which remain poorly understood and difficult to implement. So for example, it remains unclear to what extent transparency can actually be achieved in high dimensional nonlinear systems, what fairness actually consists in, and how we can assess the reliability of AI-generated content, predictions, and recommendations. To a large extent, these are philosophical questions that require a close analysis of relevant concepts, as well as a close integration of technical solutions with normative and societal constraints.

**Transparency:** ECPAI members are engaged in the project of defining normative constraints for Explainable AI, and actively collaborate with representatives from industry and research to develop methods with which to explain the behavior of opaque AI systems in many different domains, from medical applications to self-driving cars. To this end, they study typical use-cases of Explainable AI, evaluate the possibilities and limits of current explanatory practices, and participate in regulatory efforts to promote transparency in AI.

**Reliability:** ECPAI members aim to uncover and characterize the criteria that can be used to judge whether or not an AI system's recommendations are reliable.


## Funded Projects

- [NWO LTP ROBUST project](https://www.tue.nl/en/storage/biomedische-technologie/de-faculteit/news-and-events/news-overview/05-01-2023-robust-ai-program-receives-additional-eur25-million-in-funding-from-dutch-research-council) on explainability of machine vision for self-driving cars
- [NWO XS grant](https://www.tue.nl/en/news-and-events/news-overview/03-05-2023-xs-grant-for-elizabeth-oneill) on "When Computers Join the Moral Conversation"


## Activities

- Contribution to technical standard [DIN SPEC 92001-3 - Explainability](https://www.beuth.de/en/technical-rule/din-spec-92001-3/369799101)
- YouTube video on [*How to Evaluate the Reliability of a Source?*](https://www.youtube.com/watch?v=Ts8F4V0yI-M)
- Vlasta Sikimić participated in the [governmental advisory board creating the Ethical Guidelines for Safe and Reliable Use of AI in the Republic of Serbia](https://www.ai.gov.rs/vest/en/423/adopted-ethical-guidelines-for-safe-and-reliable-use-of-ai.php). 


## Associated Researchers

- Céline Budding
- Elizabeth O'Neill
- Philip J. Nickel
- Yeji Streppel
- Philippe Verreault-Julien
- Carlos Zednik
