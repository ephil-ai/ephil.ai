---
title: Cognitive Explainability
summary: 'Explainability is a critical quality requirement for AI systems. It is threatened when systems are too complex and dynamic, and it may be ensured through interpretable design or through post-hoc explanation. Philosophical work is required, however, to understand what this actually means. [(read more)](/project/explainability)'
date: "2021-12-01T00:00:00Z"
profile: false
show_date: false

# For order in list 
weight: 10

# Optional external URL for project (replaces project detail page).
external_link: ""

# Featured image
# To use, place an image named `featured.jpg/png` in your page's folder.
# Placement options: 1 = Full column width, 2 = Out-set, 3 = Screen-width
# Focal point options: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight
# Set `preview_only` to `true` to just use the image for thumbnails.
image:
 caption: Photo by <a href="https://unsplash.com/@lukejonesdesign?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Luke Jones</a> on <a href="https://unsplash.com/photos/a-close-up-of-a-computer-circuit-board-tBvF46kmwBw?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
 placement: 1
 focal_point: Smart
 preview_only: false


links:
#- icon: twitter
 # icon_pack: fab
 # name: Follow
 # url: https://twitter.com/georgecushen
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""

# Slides (optional).
#   Associate this project with Markdown slides.
#   Simply enter your slide deck's filename without extension.
#   E.g. `slides = "example-slides"` references `content/slides/example-slides.md`.
#   Otherwise, set `slides = ""`.
slides: ''
---

When AI systems are opaque, explanations can be used to help render them transparent. But whatform should such explanations take? Researchers in this cluster explore links between artificial intelligence and human cognition to transfer methodological and theoretical insights from psychology and neuroscience to advance the development and use of explainable AI. Methods from philosophy of science are used to evaluate the explanatory credentials of different XAI tools;insights from philosophy of mind and theoretical psychology are used to gauge the applicability of mental constructs to symbolic and data-driven AI systems; considerations from ethics and policy are considered to ensure that the tools being developed align with societal norms and values. Current application areas include (but are not limited to) automotive vision, natural language processing,and educational support services.

The cluster aims to not just publish philosophical research, but to also directly advance empirical research, technological development, and AI governance efforts. While empirical research on human cognition is a source of inspiration, the research being conducted in this cluster also feedsback to inform theory and methodology in psychology, neuroscience, and cognitive science more generally. Through active collaborations with researchers at e.g. TU/e's Mobile Perceptual Systemslab, researchers in this cluster also contribute to the transparency and performance of state-of-the-art AI systems. Finally, through active participation in national and international standardization, members of this cluster ensure that their research has a direct real-world impact.

## Funded Projects

- [NWO LTP ROBUST project](https://www.tue.nl/en/storage/biomedische-technologie/de-faculteit/news-and-events/news-overview/05-01-2023-robust-ai-program-receives-additional-eur25-million-in-funding-from-dutch-research-council) on explainability of machine vision for self-driving cars
- [MSCA AlignAI project](https://alignai.eu/) on explainability and fairness in large language models.


## PhD Projects

- Tacit knowledge in large language models (C. Budding)
- Schema-based representations for automotive vision (M. Ghezzi)
- Reasoning processes in LLMs and AI agents (Z. Kabadere)

## Associated Researchers

- Carlos Zednik
- CÃ©line Budding
- Michela Ghezzi
- Zeynep Kabadere
