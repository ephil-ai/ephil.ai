
@article{Sullivan:2022,
  title = {Inductive Risk, Understanding, and Opaque Machine Learning
Models},
  author = {Sullivan, Emily},
  year = {Forthcoming},
  journal = {Philosophy of Science},
  doi = {10.1017/psa.2022.62},
  abstract = {Under what conditions does machine learning (ML) model opacity inhibit the possibility of explaining and understanding phenomena? In this paper, I argue that non-epistemic values give shape to the ML opacity problem even if we keep researcher interests fixed. Treating ML models as an instance of doing model-based science to explain and understand phenomena reveals that there is (i) an \emph{external} opacity problem, where the presence of inductive risk imposes higher standards on externally validating models, and (ii) an \emph{internal} opacity problem, where greater inductive risk demands a higher level of transparency regarding the inferences the model makes.}
}


